{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T08:06:37.154189Z","iopub.execute_input":"2023-06-29T08:06:37.155105Z","iopub.status.idle":"2023-06-29T08:06:37.171305Z","shell.execute_reply.started":"2023-06-29T08:06:37.155070Z","shell.execute_reply":"2023-06-29T08:06:37.170252Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/scalex2/checkpoint.pth.tar\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:37.172978Z","iopub.execute_input":"2023-06-29T08:06:37.173346Z","iopub.status.idle":"2023-06-29T08:06:50.959328Z","shell.execute_reply.started":"2023-06-29T08:06:37.173314Z","shell.execute_reply":"2023-06-29T08:06:50.958249Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport random\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.functional as F\nimport torch\nfrom torch import nn\nfrom torchsummary import summary\nimport time\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:50.961579Z","iopub.execute_input":"2023-06-29T08:06:50.964757Z","iopub.status.idle":"2023-06-29T08:06:54.778234Z","shell.execute_reply.started":"2023-06-29T08:06:50.964715Z","shell.execute_reply":"2023-06-29T08:06:54.777294Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom progressbar import ProgressBar","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:54.779575Z","iopub.execute_input":"2023-06-29T08:06:54.780198Z","iopub.status.idle":"2023-06-29T08:06:54.812404Z","shell.execute_reply.started":"2023-06-29T08:06:54.780163Z","shell.execute_reply":"2023-06-29T08:06:54.811484Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"try:\n    import accimage\nexcept:\n    accimage = None\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef to_tensor(pic):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\n    See ``ToTensor`` for more details.\n\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    \"\"\"\n    if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n        raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n    if accimage is not None and isinstance(pic, accimage.Image):\n        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n        pic.copyto(nppic)\n        return torch.from_numpy(nppic)\n\n    # handle PIL Image\n    if pic.mode == 'I':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == 'I;16':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    elif pic.mode == 'F':\n        img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n    elif pic.mode == '1':\n        img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == 'YCbCr':\n        nchannel = 3\n    elif pic.mode == 'I;16':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\ndef normalize(tensor, mean, std):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n\n    See ``Normalize`` for more details.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channely.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    \"\"\"\n    if not _is_tensor_image(tensor):\n        raise TypeError('tensor is not a torch image.')\n    # TODO: make efficient\n    for t, m, s in zip(tensor, mean, std):\n        t.sub_(m).div_(s)\n    return tensor","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-06-29T08:06:54.814923Z","iopub.execute_input":"2023-06-29T08:06:54.815240Z","iopub.status.idle":"2023-06-29T08:06:54.834234Z","shell.execute_reply.started":"2023-06-29T08:06:54.815209Z","shell.execute_reply":"2023-06-29T08:06:54.833292Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:54.835407Z","iopub.execute_input":"2023-06-29T08:06:54.835916Z","iopub.status.idle":"2023-06-29T08:06:54.975696Z","shell.execute_reply.started":"2023-06-29T08:06:54.835882Z","shell.execute_reply":"2023-06-29T08:06:54.974805Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Utils ","metadata":{}},{"cell_type":"code","source":"def get_scale_from_dataset(dataset):\n    scale = None\n    if len(dataset) > 0:\n        lr = Image.open(dataset[0]['lr'])\n        hr = Image.open(dataset[0]['hr'])\n        dim1 = round(hr.width / lr.width)\n        dim2 = round(hr.height / lr.height)\n        scale = max(dim1, dim2)\n    return scale\n\n\ndef get_scale(lr, hr):\n    dim1 = round(hr.width / lr.width)\n    dim2 = round(hr.height / lr.height)\n    scale = max(dim1, dim2)\n    return scale\n\n\ndef resize_image(lr_image, hr_image, scale=None):\n    if scale is None:\n        scale = get_scale(lr_image, hr_image)\n    if lr_image.width * scale != hr_image.width or lr_image.height * scale != hr_image.height:\n        hr_width = lr_image.width * scale\n        hr_height = lr_image.height * scale\n        return hr_image.resize((hr_width, hr_height), resample=Image.BICUBIC)\n    return hr_image","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:54.977078Z","iopub.execute_input":"2023-06-29T08:06:54.977411Z","iopub.status.idle":"2023-06-29T08:06:54.987875Z","shell.execute_reply.started":"2023-06-29T08:06:54.977380Z","shell.execute_reply":"2023-06-29T08:06:54.986926Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class EvalDataset(Dataset):\n    def __init__(self, dataset, transform = None):\n        super(EvalDataset, self).__init__()\n        self.dataset = dataset\n        self.scale = get_scale_from_dataset(dataset)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        lr_image = Image.open(self.dataset[idx]['lr']).convert('RGB')\n        hr_image = resize_image(lr_image, Image.open(self.dataset[idx]['hr']).convert('RGB'), scale=self.scale)\n        lr = np.array(lr_image)\n        hr = np.array(hr_image)\n        lr = lr.astype(np.float32).transpose([2, 0, 1]) / 255\n        hr = hr.astype(np.float32).transpose([2, 0, 1]) / 255\n        if self.transform:\n            lr, hr = self.transform(lr, hr)\n            # label = self.transform(label)\n\n        return lr, hr\n\n    def __len__(self):\n        return len(self.dataset)\n    \n    \nclass TrainDataset(Dataset):\n    def __init__(self, dataset, transform = None, patch_size = 64):\n        super(TrainDataset, self).__init__()\n        self.dataset = dataset\n        self.patch_size = patch_size\n        self.scale = get_scale_from_dataset(dataset)\n        self.transform = transform\n    \n    @staticmethod\n    def random_crop(lr, hr, size, scale):\n        lr_left = random.randint(0, lr.shape[1] - size)\n        lr_right = lr_left + size\n        lr_top = random.randint(0, lr.shape[0] - size)\n        lr_bottom = lr_top + size\n        hr_left = lr_left * scale\n        hr_right = lr_right * scale\n        hr_top = lr_top * scale\n        hr_bottom = lr_bottom * scale\n        lr = lr[lr_top:lr_bottom, lr_left:lr_right]\n        hr = hr[hr_top:hr_bottom, hr_left:hr_right]\n        return lr, hr\n\n    @staticmethod\n    def random_horizontal_flip(lr, hr):\n        if random.random() < 0.5:\n            lr = lr[:, ::-1, :].copy()\n            hr = hr[:, ::-1, :].copy()\n        return lr, hr\n\n    @staticmethod\n    def random_vertical_flip(lr, hr):\n        if random.random() < 0.5:\n            lr = lr[::-1, :, :].copy()\n            hr = hr[::-1, :, :].copy()\n        return lr, hr\n\n    @staticmethod\n    def random_rotate_90(lr, hr):\n        if random.random() < 0.5:\n            lr = np.rot90(lr, axes=(1, 0)).copy()\n            hr = np.rot90(hr, axes=(1, 0)).copy()\n        return lr, hr\n\n    def __getitem__(self, idx):\n        lr_image = Image.open(self.dataset[idx]['lr']).convert('RGB')\n        hr_image = resize_image(lr_image, Image.open(self.dataset[idx]['hr']).convert('RGB'), scale=self.scale)\n        lr = np.array(lr_image)\n        hr = np.array(hr_image)\n        lr, hr = self.random_crop(lr, hr, self.patch_size, self.scale)\n        lr, hr = self.random_horizontal_flip(lr, hr)\n        lr, hr = self.random_vertical_flip(lr, hr)\n        lr, hr = self.random_rotate_90(lr, hr)\n        lr = lr.astype(np.float32).transpose([2, 0, 1]) / 255\n        hr = hr.astype(np.float32).transpose([2, 0, 1]) / 255\n        \n        if self.transform:\n            lr, hr = self.transform(lr, hr)\n\n        return lr, hr\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:54.989237Z","iopub.execute_input":"2023-06-29T08:06:54.989854Z","iopub.status.idle":"2023-06-29T08:06:55.167930Z","shell.execute_reply.started":"2023-06-29T08:06:54.989824Z","shell.execute_reply":"2023-06-29T08:06:55.166595Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install super_image","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:06:55.169117Z","iopub.execute_input":"2023-06-29T08:06:55.169781Z","iopub.status.idle":"2023-06-29T08:07:06.261090Z","shell.execute_reply.started":"2023-06-29T08:06:55.169747Z","shell.execute_reply":"2023-06-29T08:07:06.259911Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting super_image\n  Downloading super_image-0.1.7-py3-none-any.whl (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h5py>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (3.8.0)\nRequirement already satisfied: huggingface-hub>=0.0.13 in /opt/conda/lib/python3.10/site-packages (from super_image) (0.15.1)\nRequirement already satisfied: opencv-python>=4.5.2.54 in /opt/conda/lib/python3.10/site-packages (from super_image) (4.7.0.72)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (2.0.0)\nRequirement already satisfied: torchvision>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (0.15.1)\nRequirement already satisfied: tqdm>=4.61.2 in /opt/conda/lib/python3.10/site-packages (from super_image) (4.64.1)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from h5py>=3.1.0->super_image) (1.23.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (3.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (2.28.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (3.1.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.10.0->super_image) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.0.13->super_image) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->super_image) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->super_image) (1.3.0)\nInstalling collected packages: super_image\nSuccessfully installed super_image-0.1.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from super_image.models.edsr.configuration_edsr import EdsrConfig\nfrom super_image.modeling_utils import (\n    default_conv,\n    MeanShift,\n    Upsampler,PreTrainedModel\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:07:06.264937Z","iopub.execute_input":"2023-06-29T08:07:06.265269Z","iopub.status.idle":"2023-06-29T08:07:06.435230Z","shell.execute_reply.started":"2023-06-29T08:07:06.265238Z","shell.execute_reply":"2023-06-29T08:07:06.434340Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Model ","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(\n            self, conv, n_feats, kernel_size,\n            bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(ResBlock, self).__init__()\n        m = []\n        for i in range(2):\n            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n            if bn:\n                m.append(nn.BatchNorm2d(n_feats))\n            if i == 0:\n                m.append(act)\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.body(x).mul(self.res_scale)\n        res += x\n\n        return res\n\n\nclass edsr(PreTrainedModel):\n    \n#     config_class = EdsrConfig\n    \n    def __init__(self, args, conv=default_conv):\n        super(edsr, self).__init__(args)\n\n        self.args = args\n        n_resblocks = args.n_resblocks\n        n_feats = args.n_feats\n        n_colors = args.n_colors\n        kernel_size = 3\n        scale = args.scale\n        rgb_range = args.rgb_range\n        act = nn.ReLU(True)\n        self.sub_mean = MeanShift(rgb_range, rgb_mean=args.rgb_mean, rgb_std=args.rgb_std)  # standardize input\n        self.add_mean = MeanShift(rgb_range, sign=1, rgb_mean=args.rgb_mean, rgb_std=args.rgb_std)  # restore output\n\n        # define head module, channels: 3->64\n        m_head = [conv(n_colors, n_feats, kernel_size)]\n\n        # define body module, channels: 64->64\n        m_body = [\n            ResBlock(\n                conv, n_feats, kernel_size, act=act, res_scale=args.res_scale\n            ) for _ in range(n_resblocks)\n        ]\n        m_body.append(conv(n_feats, n_feats, kernel_size))\n\n        self.head = nn.Sequential(*m_head)\n        self.body = nn.Sequential(*m_body)\n\n        if args.no_upsampling:\n            self.out_dim = n_feats\n        else:\n            self.out_dim = args.n_colors\n            # define tail module\n            m_tail = [\n                Upsampler(conv, scale, n_feats, act=False),\n                conv(n_feats, n_colors, kernel_size)\n            ]\n            self.tail = nn.Sequential(*m_tail)\n\n    def forward(self, x):\n        x = self.head(x)\n\n        res = self.body(x)\n        res += x\n\n        if self.args.no_upsampling:\n            x = res\n        else:\n            x = self.tail(res)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:07:06.440088Z","iopub.execute_input":"2023-06-29T08:07:06.440375Z","iopub.status.idle":"2023-06-29T08:07:06.454375Z","shell.execute_reply.started":"2023-06-29T08:07:06.440351Z","shell.execute_reply":"2023-06-29T08:07:06.453505Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:07:06.455696Z","iopub.execute_input":"2023-06-29T08:07:06.456248Z","iopub.status.idle":"2023-06-29T08:07:06.489942Z","shell.execute_reply.started":"2023-06-29T08:07:06.456212Z","shell.execute_reply":"2023-06-29T08:07:06.488900Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"# train_dir = 'data/train'\n# val_dir = 'data/validation'\n\n# train_transform = Compose([\n#                             Normalize([0.449, 0.438, 0.404],\n#                                       [1.0, 1.0, 1.0])])\n\n# valid_transform = Compose([\n#                             Normalize([0.440, 0.435, 0.403],\n#                                       [1.0, 1.0, 1.0])])\n\nt_set = load_dataset('eugenesiow/Div2k', 'bicubic_x2', split='train')\ne_set = load_dataset('eugenesiow/Div2k', 'bicubic_x2', split='validation')\n\n# trainset = TrainDataset(t_set, transform=train_transform)\n# validset = EvalDataset(e_set, transform=valid_transform)\n\n\n# trainset = DIV2K_x2(root_dir=train_dir, im_size=40, scale=2, transform=train_transforms)\n# validset = DIV2K_x2(root_dir=val_dir, im_size=40, scale=2, transform=valid_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:07:06.491385Z","iopub.execute_input":"2023-06-29T08:07:06.491886Z","iopub.status.idle":"2023-06-29T08:09:26.123476Z","shell.execute_reply.started":"2023-06-29T08:07:06.491854Z","shell.execute_reply":"2023-06-29T08:09:26.122578Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96cdd654b9b940daacef0dab8a423c8a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset div2k/bicubic_x2 to /root/.cache/huggingface/datasets/eugenesiow___div2k/bicubic_x2/2.0.0/d7599f94c7e662a3eed3547efc7efa52b2ed71082b40fc2e42a693870e35b677...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7899b6639b7431ca25cf255f073a6b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/925M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1fb1db66c5a42e3b0d35e65e255fa77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/118M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01b6c5f061db41a4ad5526318e38ff24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc77984b6bf249098158cab34f0d8ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/449M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63d39a09b3c42a2a0a04d2b387c21a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985d2ded123a48f99e71ba5625fc16cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset div2k downloaded and prepared to /root/.cache/huggingface/datasets/eugenesiow___div2k/bicubic_x2/2.0.0/d7599f94c7e662a3eed3547efc7efa52b2ed71082b40fc2e42a693870e35b677. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainset = TrainDataset(t_set)\nvalidset = EvalDataset(e_set)\n\ntrainloader = DataLoader(trainset, batch_size=8, shuffle=True)\nvalidloader = DataLoader(validset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:09:26.128930Z","iopub.execute_input":"2023-06-29T08:09:26.129272Z","iopub.status.idle":"2023-06-29T08:09:26.164892Z","shell.execute_reply.started":"2023-06-29T08:09:26.129234Z","shell.execute_reply":"2023-06-29T08:09:26.162961Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint.pth.tar'\n    torch.save(state, filename)\n    \ndef adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[0]['lr'],))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:09:26.167300Z","iopub.execute_input":"2023-06-29T08:09:26.168407Z","iopub.status.idle":"2023-06-29T08:09:27.971935Z","shell.execute_reply.started":"2023-06-29T08:09:26.168372Z","shell.execute_reply":"2023-06-29T08:09:27.970837Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:09:27.973111Z","iopub.execute_input":"2023-06-29T08:09:27.973537Z","iopub.status.idle":"2023-06-29T08:09:27.985049Z","shell.execute_reply.started":"2023-06-29T08:09:27.973497Z","shell.execute_reply":"2023-06-29T08:09:27.984101Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"criterion = nn.L1Loss()\nscale = 2\nepochs = 25\nprint_every = 5\ntrain_loss = 0\nbatch_num = 0\ndecay_lr_at = 11, 20  # decay learning rate after these many iterations\ndecay_lr_to = 0.15","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:30:28.011524Z","iopub.execute_input":"2023-06-29T11:30:28.012255Z","iopub.status.idle":"2023-06-29T11:30:28.017369Z","shell.execute_reply.started":"2023-06-29T11:30:28.012221Z","shell.execute_reply":"2023-06-29T11:30:28.016306Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from super_image.trainer_utils import EvalPrediction\nfrom super_image.utils.metrics import compute_metrics\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:30:28.422785Z","iopub.execute_input":"2023-06-29T11:30:28.423467Z","iopub.status.idle":"2023-06-29T11:30:28.429805Z","shell.execute_reply.started":"2023-06-29T11:30:28.423436Z","shell.execute_reply":"2023-06-29T11:30:28.428426Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"/kaggle/working/checkpoint.pth.tar\"\n\nbest_metric = 0\nbest_epoch = 0\n\ndef train(train_loader,valid_loader, model, criterion, optimizer, epoch):\n    \n    global best_metric, best_epoch\n    losses = AverageMeter()\n    \n    for i, (img, label) in enumerate(train_loader):\n        \n        start = time.time()\n\n        img, label = img.to(device), label.to(device)\n        pred = model(img)\n        # print(pred.shape, label.shape)\n        loss = criterion(pred, label)\n        \n        losses.update(loss.item(), img.size(0))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Print status\n        if i % print_every == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Training Time {3:.3f} \\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n                                                                  (time.time()-start)*print_every, loss=losses))\n\n    with torch.no_grad():\n        \n        model.eval()\n        val_losses = AverageMeter()\n        epoch_psnr = AverageMeter()\n        epoch_ssim = AverageMeter()\n        \n        for i, (val_inputs, val_labels) in enumerate(valid_loader):\n            \n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n            val_preds = model(val_inputs)\n            val_loss = criterion(val_preds, val_labels)\n            \n            val_losses.update(val_loss.item(), val_inputs.size(0))\n            \n            metrics = compute_metrics(EvalPrediction(predictions=val_preds, labels=val_labels), scale=scale)\n\n            epoch_psnr.update(metrics['psnr'], val_inputs.size(0))\n            epoch_ssim.update(metrics['ssim'], val_inputs.size(0))\n\n        print(f'Validation Loss:{val_losses.avg:.2f}      eval psnr: {epoch_psnr.avg:.2f}     ssim: {epoch_ssim.avg:.4f}')\n\n        if epoch_psnr.avg > best_metric:\n            best_epoch = epoch\n            best_metric = epoch_psnr.avg\n\n            print(f'best epoch: {epoch}, psnr: {epoch_psnr.avg:.6f}, ssim: {epoch_ssim.avg:.6f}')\n            \n            # Save checkpoint\n            print(\"Saving checkpoint epoch:\", epoch)\n            save_checkpoint(epoch, model, optimizer)\n\n#         print('Epoch : {}/{}'.format(epoch_num, epochs))\n#         print('Training Loss : {:.4f}'.format(losses.avg))\n#         print('Validation Loss: {:.4f}'.format(val_losses.avg))","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:30:28.729512Z","iopub.execute_input":"2023-06-29T11:30:28.729916Z","iopub.status.idle":"2023-06-29T11:30:28.743198Z","shell.execute_reply.started":"2023-06-29T11:30:28.729887Z","shell.execute_reply":"2023-06-29T11:30:28.742257Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Training.\n    \"\"\"\n    global start_epoch, label_map, epoch, checkpoint, decay_lr_at, optimizer, criterion, scale\n\n    # Initialize model or load checkpoint\n    if checkpoint is None:\n        config = EdsrConfig(\n        scale=scale,                               \n        n_resblocks=32,\n        n_feats=256\n    )\n        start_epoch = 0\n        model = edsr(config)\n        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n        \n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n        model = checkpoint['model']\n        optimizer = checkpoint['optimizer']\n\n    # Move to default device\n    model = model.to(device)\n    criterion = criterion\n\n    print(\"Number of epochs: \", epochs)\n    \n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # Decay learning rate at particular epochs\n        if epoch in decay_lr_at:\n            adjust_learning_rate(optimizer, decay_lr_to)\n\n        # One epoch's training\n        train(train_loader=trainloader,\n              valid_loader = validloader,\n              model=model,\n              criterion=criterion,\n              optimizer=optimizer,\n              epoch=epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:30:28.968455Z","iopub.execute_input":"2023-06-29T11:30:28.969401Z","iopub.status.idle":"2023-06-29T11:30:28.978843Z","shell.execute_reply.started":"2023-06-29T11:30:28.969366Z","shell.execute_reply":"2023-06-29T11:30:28.977836Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:30:29.231010Z","iopub.execute_input":"2023-06-29T11:30:29.231723Z","iopub.status.idle":"2023-06-29T11:35:58.249634Z","shell.execute_reply.started":"2023-06-29T11:30:29.231688Z","shell.execute_reply":"2023-06-29T11:35:58.248164Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"\nLoaded checkpoint from epoch 20.\n\nNumber of epochs:  25\nDECAYING learning rate.\n The new LR is 0.000004\n\nEpoch: [20][0/100]\tTraining Time 1.291 \tLoss 0.0100 (0.0100)\t\nEpoch: [20][5/100]\tTraining Time 1.214 \tLoss 0.0288 (0.0158)\t\nEpoch: [20][10/100]\tTraining Time 1.215 \tLoss 0.0153 (0.0154)\t\nEpoch: [20][15/100]\tTraining Time 1.211 \tLoss 0.0103 (0.0148)\t\nEpoch: [20][20/100]\tTraining Time 1.212 \tLoss 0.0268 (0.0153)\t\nEpoch: [20][25/100]\tTraining Time 1.214 \tLoss 0.0204 (0.0164)\t\nEpoch: [20][30/100]\tTraining Time 1.214 \tLoss 0.0181 (0.0164)\t\nEpoch: [20][35/100]\tTraining Time 1.216 \tLoss 0.0165 (0.0167)\t\nEpoch: [20][40/100]\tTraining Time 1.211 \tLoss 0.0069 (0.0165)\t\nEpoch: [20][45/100]\tTraining Time 1.211 \tLoss 0.0114 (0.0165)\t\nEpoch: [20][50/100]\tTraining Time 1.215 \tLoss 0.0163 (0.0165)\t\nEpoch: [20][55/100]\tTraining Time 1.213 \tLoss 0.0161 (0.0165)\t\nEpoch: [20][60/100]\tTraining Time 1.213 \tLoss 0.0169 (0.0167)\t\nEpoch: [20][65/100]\tTraining Time 1.215 \tLoss 0.0133 (0.0166)\t\nEpoch: [20][70/100]\tTraining Time 1.212 \tLoss 0.0137 (0.0164)\t\nEpoch: [20][75/100]\tTraining Time 1.212 \tLoss 0.0171 (0.0163)\t\nEpoch: [20][80/100]\tTraining Time 1.213 \tLoss 0.0186 (0.0161)\t\nEpoch: [20][85/100]\tTraining Time 1.214 \tLoss 0.0287 (0.0162)\t\nEpoch: [20][90/100]\tTraining Time 1.213 \tLoss 0.0144 (0.0162)\t\nEpoch: [20][95/100]\tTraining Time 1.212 \tLoss 0.0173 (0.0161)\t\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[35], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     adjust_learning_rate(optimizer, decay_lr_to)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# One epoch's training\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[34], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (val_inputs, val_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_loader):\n\u001b[1;32m     42\u001b[0m     val_inputs, val_labels \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device), val_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m     val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(val_preds, val_labels)\n\u001b[1;32m     46\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mupdate(val_loss\u001b[38;5;241m.\u001b[39mitem(), val_inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36medsr.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/pixelshuffle.py:54\u001b[0m, in \u001b[0;36mPixelShuffle.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupscale_factor\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.97 GiB (GPU 0; 15.90 GiB total capacity; 10.15 GiB already allocated; 3.78 GiB free; 11.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.97 GiB (GPU 0; 15.90 GiB total capacity; 10.15 GiB already allocated; 3.78 GiB free; 11.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:28:57.965029Z","iopub.execute_input":"2023-06-29T11:28:57.965424Z","iopub.status.idle":"2023-06-29T11:28:58.348877Z","shell.execute_reply.started":"2023-06-29T11:28:57.965392Z","shell.execute_reply":"2023-06-29T11:28:58.347598Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"7042"},"metadata":{}}]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n\n!tar -czf checkpoint.pth.tar \n\nfrom IPython.display import FileLink\n\nFileLink(r'checkpoint.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2023-06-29T11:29:02.037879Z","iopub.execute_input":"2023-06-29T11:29:02.038346Z","iopub.status.idle":"2023-06-29T11:29:03.124458Z","shell.execute_reply.started":"2023-06-29T11:29:02.038308Z","shell.execute_reply":"2023-06-29T11:29:03.123148Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"tar: Cowardly refusing to create an empty archive\nTry 'tar --help' or 'tar --usage' for more information.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoint.pth.tar","text/html":"<a href='checkpoint.pth.tar' target='_blank'>checkpoint.pth.tar</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# x4: 11,18\n# x3:\n# x2: ","metadata":{"execution":{"iopub.status.busy":"2023-06-29T08:09:28.736516Z","iopub.status.idle":"2023-06-29T08:09:28.737508Z","shell.execute_reply.started":"2023-06-29T08:09:28.737270Z","shell.execute_reply":"2023-06-29T08:09:28.737292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
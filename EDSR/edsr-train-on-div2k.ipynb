{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-24T18:18:02.406269Z","iopub.execute_input":"2023-06-24T18:18:02.406659Z","iopub.status.idle":"2023-06-24T18:18:02.421415Z","shell.execute_reply.started":"2023-06-24T18:18:02.406629Z","shell.execute_reply":"2023-06-24T18:18:02.420068Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:02.714418Z","iopub.execute_input":"2023-06-24T18:18:02.716432Z","iopub.status.idle":"2023-06-24T18:18:14.932710Z","shell.execute_reply.started":"2023-06-24T18:18:02.716394Z","shell.execute_reply":"2023-06-24T18:18:14.931366Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport os\nimport random\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.functional as F\nimport torch\nfrom torch import nn\nfrom torchsummary import summary\nimport time\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:14.935831Z","iopub.execute_input":"2023-06-24T18:18:14.936129Z","iopub.status.idle":"2023-06-24T18:18:19.081388Z","shell.execute_reply.started":"2023-06-24T18:18:14.936102Z","shell.execute_reply":"2023-06-24T18:18:19.080437Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom progressbar import ProgressBar","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.082778Z","iopub.execute_input":"2023-06-24T18:18:19.083858Z","iopub.status.idle":"2023-06-24T18:18:19.112362Z","shell.execute_reply.started":"2023-06-24T18:18:19.083820Z","shell.execute_reply":"2023-06-24T18:18:19.111529Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"try:\n    import accimage\nexcept:\n    accimage = None\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef to_tensor(pic):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\n    See ``ToTensor`` for more details.\n\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    \"\"\"\n    if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n        raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backward compatibility\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n    if accimage is not None and isinstance(pic, accimage.Image):\n        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n        pic.copyto(nppic)\n        return torch.from_numpy(nppic)\n\n    # handle PIL Image\n    if pic.mode == 'I':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == 'I;16':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    elif pic.mode == 'F':\n        img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n    elif pic.mode == '1':\n        img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == 'YCbCr':\n        nchannel = 3\n    elif pic.mode == 'I;16':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\ndef normalize(tensor, mean, std):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n\n    See ``Normalize`` for more details.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channely.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    \"\"\"\n    if not _is_tensor_image(tensor):\n        raise TypeError('tensor is not a torch image.')\n    # TODO: make efficient\n    for t, m, s in zip(tensor, mean, std):\n        t.sub_(m).div_(s)\n    return tensor","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-06-24T18:18:19.115086Z","iopub.execute_input":"2023-06-24T18:18:19.115446Z","iopub.status.idle":"2023-06-24T18:18:19.134792Z","shell.execute_reply.started":"2023-06-24T18:18:19.115413Z","shell.execute_reply":"2023-06-24T18:18:19.133688Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.136750Z","iopub.execute_input":"2023-06-24T18:18:19.137145Z","iopub.status.idle":"2023-06-24T18:18:19.298507Z","shell.execute_reply.started":"2023-06-24T18:18:19.137111Z","shell.execute_reply":"2023-06-24T18:18:19.296461Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_scale_from_dataset(dataset):\n    scale = None\n    if len(dataset) > 0:\n        lr = Image.open(dataset[0]['lr'])\n        hr = Image.open(dataset[0]['hr'])\n        dim1 = round(hr.width / lr.width)\n        dim2 = round(hr.height / lr.height)\n        scale = max(dim1, dim2)\n    return scale\n\n\ndef get_scale(lr, hr):\n    dim1 = round(hr.width / lr.width)\n    dim2 = round(hr.height / lr.height)\n    scale = max(dim1, dim2)\n    return scale\n\n\ndef resize_image(lr_image, hr_image, scale=None):\n    if scale is None:\n        scale = get_scale(lr_image, hr_image)\n    if lr_image.width * scale != hr_image.width or lr_image.height * scale != hr_image.height:\n        hr_width = lr_image.width * scale\n        hr_height = lr_image.height * scale\n        return hr_image.resize((hr_width, hr_height), resample=Image.BICUBIC)\n    return hr_image","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.302786Z","iopub.execute_input":"2023-06-24T18:18:19.305396Z","iopub.status.idle":"2023-06-24T18:18:19.317434Z","shell.execute_reply.started":"2023-06-24T18:18:19.305358Z","shell.execute_reply":"2023-06-24T18:18:19.316469Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EvalDataset(Dataset):\n    def __init__(self, dataset, transform = None):\n        super(EvalDataset, self).__init__()\n        self.dataset = dataset\n        self.scale = get_scale_from_dataset(dataset)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        lr_image = Image.open(self.dataset[idx]['lr']).convert('RGB')\n        hr_image = resize_image(lr_image, Image.open(self.dataset[idx]['hr']).convert('RGB'), scale=self.scale)\n        lr = np.array(lr_image)\n        hr = np.array(hr_image)\n        lr = lr.astype(np.float32).transpose([2, 0, 1]) / 255\n        hr = hr.astype(np.float32).transpose([2, 0, 1]) / 255\n        if self.transform:\n            lr, hr = self.transform(lr, hr)\n            # label = self.transform(label)\n\n        return lr, hr\n\n    def __len__(self):\n        return len(self.dataset)\n    \n    \nclass TrainDataset(Dataset):\n    def __init__(self, dataset, transform = None, patch_size = 64):\n        super(TrainDataset, self).__init__()\n        self.dataset = dataset\n        self.patch_size = patch_size\n        self.scale = get_scale_from_dataset(dataset)\n        self.transform = transform\n    \n    @staticmethod\n    def random_crop(lr, hr, size, scale):\n        lr_left = random.randint(0, lr.shape[1] - size)\n        lr_right = lr_left + size\n        lr_top = random.randint(0, lr.shape[0] - size)\n        lr_bottom = lr_top + size\n        hr_left = lr_left * scale\n        hr_right = lr_right * scale\n        hr_top = lr_top * scale\n        hr_bottom = lr_bottom * scale\n        lr = lr[lr_top:lr_bottom, lr_left:lr_right]\n        hr = hr[hr_top:hr_bottom, hr_left:hr_right]\n        return lr, hr\n\n    @staticmethod\n    def random_horizontal_flip(lr, hr):\n        if random.random() < 0.5:\n            lr = lr[:, ::-1, :].copy()\n            hr = hr[:, ::-1, :].copy()\n        return lr, hr\n\n    @staticmethod\n    def random_vertical_flip(lr, hr):\n        if random.random() < 0.5:\n            lr = lr[::-1, :, :].copy()\n            hr = hr[::-1, :, :].copy()\n        return lr, hr\n\n    @staticmethod\n    def random_rotate_90(lr, hr):\n        if random.random() < 0.5:\n            lr = np.rot90(lr, axes=(1, 0)).copy()\n            hr = np.rot90(hr, axes=(1, 0)).copy()\n        return lr, hr\n\n    def __getitem__(self, idx):\n        lr_image = Image.open(self.dataset[idx]['lr']).convert('RGB')\n        hr_image = resize_image(lr_image, Image.open(self.dataset[idx]['hr']).convert('RGB'), scale=self.scale)\n        lr = np.array(lr_image)\n        hr = np.array(hr_image)\n        lr, hr = self.random_crop(lr, hr, self.patch_size, self.scale)\n        lr, hr = self.random_horizontal_flip(lr, hr)\n        lr, hr = self.random_vertical_flip(lr, hr)\n        lr, hr = self.random_rotate_90(lr, hr)\n        lr = lr.astype(np.float32).transpose([2, 0, 1]) / 255\n        hr = hr.astype(np.float32).transpose([2, 0, 1]) / 255\n        \n        if self.transform:\n            lr, hr = self.transform(lr, hr)\n\n        return lr, hr\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.322711Z","iopub.execute_input":"2023-06-24T18:18:19.325316Z","iopub.status.idle":"2023-06-24T18:18:19.569236Z","shell.execute_reply.started":"2023-06-24T18:18:19.325283Z","shell.execute_reply":"2023-06-24T18:18:19.568013Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# class DIV2K_x2(Dataset):\n#     def __init__(self, root_dir, im_size, scale, transform=None):\n\n#         self.root_dir = root_dir\n#         self.im_size = im_size\n#         self.scale = scale\n#         self.transform = transform\n\n#         images = []\n#         labels = []\n#         for file in os.listdir(self.root_dir + '/img'):\n#             if not file.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff')):\n#                 continue\n#             images.append(file)\n#         images.sort()\n\n#         for file in os.listdir(self.root_dir + '/label'):\n#             if not file.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff')):\n#                 continue\n#             labels.append(file)\n#         labels.sort()\n\n#         self.images = images\n#         self.labels = labels\n\n#     def __len__(self):\n\n#         return len(self.images)\n\n#     def __getitem__(self, idx):\n\n#         if torch.is_tensor(idx):\n#             idx = idx.to_list()\n\n#         img_path = os.path.join(self.root_dir + '/img', self.images[idx])\n#         label_path = os.path.join(self.root_dir + '/label', self.labels[idx])\n\n#         img = Image.open(img_path)\n#         img = img.resize((int(self.im_size / self.scale), int(self.im_size / self.scale)))\n#         label = Image.open(label_path)\n#         label = label.resize((int(self.im_size), int(self.im_size)))\n\n#         if self.transform:\n#             img, label = self.transform(img, label)\n#             # label = self.transform(label)\n\n#         return img, label","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.575395Z","iopub.execute_input":"2023-06-24T18:18:19.577797Z","iopub.status.idle":"2023-06-24T18:18:19.585990Z","shell.execute_reply.started":"2023-06-24T18:18:19.577762Z","shell.execute_reply":"2023-06-24T18:18:19.584959Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\nclass RandomHorizontalFlip(object):\n    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, label):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be flipped.\n\n        Returns:\n            PIL Image: Randomly flipped image.\n        \"\"\"\n        if random.random() < self.p:\n            return img.transpose(Image.FLIP_LEFT_RIGHT), label.transpose(Image.FLIP_LEFT_RIGHT)\n        return img, label\n\n\nclass RandomVerticalFlip(object):\n    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    \"\"\"\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, label):\n        \"\"\"\n        Args:\n            img (PIL Image): Image to be flipped.\n\n        Returns:\n            PIL Image: Randomly flipped image.\n        \"\"\"\n        if random.random() < self.p:\n            return img.transpose(Image.FLIP_TOP_BOTTOM), label.transpose(Image.FLIP_TOP_BOTTOM)\n        return img, label\n\n\nclass Normalize(object):\n    \"\"\"Normalize a tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    \"\"\"\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img, label):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(img, self.mean, self.std), normalize(label, self.mean, self.std)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\n\nclass ToTensor(object):\n    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n\n    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    \"\"\"\n\n    def __call__(self, img, label):\n        \"\"\"\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        \"\"\"\n        return F.to_tensor(img), F.to_tensor(label)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass Compose(object):\n    \"\"\"Composes several transforms together.\n\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    \"\"\"\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, label):\n        for t in self.transforms:\n            img, label = t(img, label)\n            # label = t(label)\n        return img, label\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.591108Z","iopub.execute_input":"2023-06-24T18:18:19.593720Z","iopub.status.idle":"2023-06-24T18:18:19.616494Z","shell.execute_reply.started":"2023-06-24T18:18:19.593688Z","shell.execute_reply":"2023-06-24T18:18:19.615572Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# n_feat = 256\n# kernel_size = 3\n\n\n# class _Res_Block(nn.Module):\n#     def __init__(self):\n#         super(_Res_Block, self).__init__()\n\n#         self.res_conv = nn.Conv2d(n_feat, n_feat, kernel_size, padding=1)\n#         self.relu = nn.ReLU()\n\n#     def forward(self, x):\n\n#         y = self.relu(self.res_conv(x))\n#         y = self.res_conv(y)\n#         y *= 0.1\n#         y = torch.add(y, x)\n#         return y\n\n\n# class edsr(nn.Module):\n#     def __init__(self):\n#         super(edsr, self).__init__()\n\n#         in_ch = 3\n#         num_blocks = 32\n\n#         self.conv1 = nn.Conv2d(in_ch, n_feat, kernel_size, padding=1)\n#         self.conv_up = nn.Conv2d(n_feat, n_feat * 4, kernel_size, padding=1)\n#         self.conv_out = nn.Conv2d(n_feat, in_ch, kernel_size, padding=1)\n\n#         self.body = self.make_layer(_Res_Block, num_blocks)\n\n#         self.upsample = nn.Sequential(self.conv_up, nn.PixelShuffle(2))\n\n#     def make_layer(self, block, layers):\n#         res_block = []\n#         for _ in range(layers):\n#             res_block.append(block())\n#         return nn.Sequential(*res_block)\n\n#     def forward(self, x):\n\n#         out = self.conv1(x)\n#         out = self.body(out)\n#         out = self.upsample(out)\n#         out = self.conv_out(out)\n\n#         return out","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-06-24T18:18:19.626330Z","iopub.execute_input":"2023-06-24T18:18:19.628585Z","iopub.status.idle":"2023-06-24T18:18:19.635740Z","shell.execute_reply.started":"2023-06-24T18:18:19.628553Z","shell.execute_reply":"2023-06-24T18:18:19.634743Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!pip install super_image","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:19.640458Z","iopub.execute_input":"2023-06-24T18:18:19.643384Z","iopub.status.idle":"2023-06-24T18:18:34.942497Z","shell.execute_reply.started":"2023-06-24T18:18:19.643351Z","shell.execute_reply":"2023-06-24T18:18:34.941376Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting super_image\n  Downloading super_image-0.1.7-py3-none-any.whl (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h5py>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (3.8.0)\nRequirement already satisfied: huggingface-hub>=0.0.13 in /opt/conda/lib/python3.10/site-packages (from super_image) (0.15.1)\nRequirement already satisfied: opencv-python>=4.5.2.54 in /opt/conda/lib/python3.10/site-packages (from super_image) (4.7.0.72)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (2.0.0)\nRequirement already satisfied: torchvision>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from super_image) (0.15.1)\nRequirement already satisfied: tqdm>=4.61.2 in /opt/conda/lib/python3.10/site-packages (from super_image) (4.64.1)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from h5py>=3.1.0->super_image) (1.23.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (3.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (2.28.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.0.13->super_image) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->super_image) (3.1.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.10.0->super_image) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.0.13->super_image) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->super_image) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.0.13->super_image) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->super_image) (1.3.0)\nInstalling collected packages: super_image\nSuccessfully installed super_image-0.1.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from super_image.models.edsr.configuration_edsr import EdsrConfig\nfrom super_image.modeling_utils import (\n    default_conv,\n    MeanShift,\n    Upsampler,PreTrainedModel\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:34.944543Z","iopub.execute_input":"2023-06-24T18:18:34.944909Z","iopub.status.idle":"2023-06-24T18:18:35.135012Z","shell.execute_reply.started":"2023-06-24T18:18:34.944873Z","shell.execute_reply":"2023-06-24T18:18:35.133704Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(\n            self, conv, n_feats, kernel_size,\n            bias=True, bn=False, act=nn.ReLU(True), res_scale=1):\n\n        super(ResBlock, self).__init__()\n        m = []\n        for i in range(2):\n            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n            if bn:\n                m.append(nn.BatchNorm2d(n_feats))\n            if i == 0:\n                m.append(act)\n\n        self.body = nn.Sequential(*m)\n        self.res_scale = res_scale\n\n    def forward(self, x):\n        res = self.body(x).mul(self.res_scale)\n        res += x\n\n        return res\n\n\nclass edsr(PreTrainedModel):\n    \n#     config_class = EdsrConfig\n    \n    def __init__(self, args, conv=default_conv):\n        super(edsr, self).__init__(args)\n\n        self.args = args\n        n_resblocks = args.n_resblocks\n        n_feats = args.n_feats\n        n_colors = args.n_colors\n        kernel_size = 3\n        scale = args.scale\n        rgb_range = args.rgb_range\n        act = nn.ReLU(True)\n        self.sub_mean = MeanShift(rgb_range, rgb_mean=args.rgb_mean, rgb_std=args.rgb_std)  # standardize input\n        self.add_mean = MeanShift(rgb_range, sign=1, rgb_mean=args.rgb_mean, rgb_std=args.rgb_std)  # restore output\n\n        # define head module, channels: 3->64\n        m_head = [conv(n_colors, n_feats, kernel_size)]\n\n        # define body module, channels: 64->64\n        m_body = [\n            ResBlock(\n                conv, n_feats, kernel_size, act=act, res_scale=args.res_scale\n            ) for _ in range(n_resblocks)\n        ]\n        m_body.append(conv(n_feats, n_feats, kernel_size))\n\n        self.head = nn.Sequential(*m_head)\n        self.body = nn.Sequential(*m_body)\n\n        if args.no_upsampling:\n            self.out_dim = n_feats\n        else:\n            self.out_dim = args.n_colors\n            # define tail module\n            m_tail = [\n                Upsampler(conv, scale, n_feats, act=False),\n                conv(n_feats, n_colors, kernel_size)\n            ]\n            self.tail = nn.Sequential(*m_tail)\n\n    def forward(self, x):\n        x = self.head(x)\n\n        res = self.body(x)\n        res += x\n\n        if self.args.no_upsampling:\n            x = res\n        else:\n            x = self.tail(res)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:35.136520Z","iopub.execute_input":"2023-06-24T18:18:35.136856Z","iopub.status.idle":"2023-06-24T18:18:35.151540Z","shell.execute_reply.started":"2023-06-24T18:18:35.136823Z","shell.execute_reply":"2023-06-24T18:18:35.150663Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:35.153264Z","iopub.execute_input":"2023-06-24T18:18:35.153943Z","iopub.status.idle":"2023-06-24T18:18:35.187285Z","shell.execute_reply.started":"2023-06-24T18:18:35.153912Z","shell.execute_reply":"2023-06-24T18:18:35.186388Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# train_dir = 'data/train'\n# val_dir = 'data/validation'\n\ntrain_transform = Compose([\n                            Normalize([0.449, 0.438, 0.404],\n                                      [1.0, 1.0, 1.0])])\n\nvalid_transform = Compose([\n                            Normalize([0.440, 0.435, 0.403],\n                                      [1.0, 1.0, 1.0])])\n\nt_set = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\ne_set = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation')\n\n# trainset = TrainDataset(t_set, transform=train_transform)\n# validset = EvalDataset(e_set, transform=valid_transform)\n\n\n# trainset = DIV2K_x2(root_dir=train_dir, im_size=40, scale=2, transform=train_transforms)\n# validset = DIV2K_x2(root_dir=val_dir, im_size=40, scale=2, transform=valid_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:18:35.190547Z","iopub.execute_input":"2023-06-24T18:18:35.192370Z","iopub.status.idle":"2023-06-24T18:23:13.373922Z","shell.execute_reply.started":"2023-06-24T18:18:35.192336Z","shell.execute_reply":"2023-06-24T18:23:13.372936Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e346d51061b34131b9ef33ca32664552"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset div2k/bicubic_x4 to /root/.cache/huggingface/datasets/eugenesiow___div2k/bicubic_x4/2.0.0/d7599f94c7e662a3eed3547efc7efa52b2ed71082b40fc2e42a693870e35b677...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb34be56eae4dcc923d723c302444f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/247M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b4e19a8b704ccdaefbee668126c028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/31.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a82c8bcf8d9a42e0b85e4211eff7aa25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.53G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10bfb8c259a4ac7828c9aedc21323ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/449M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03a51e21e714ba094dcb67a1754de29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a0e57a586d47bc8eb1af3ba0a8f8f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset div2k downloaded and prepared to /root/.cache/huggingface/datasets/eugenesiow___div2k/bicubic_x4/2.0.0/d7599f94c7e662a3eed3547efc7efa52b2ed71082b40fc2e42a693870e35b677. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainset = TrainDataset(t_set)\nvalidset = EvalDataset(e_set)\n\ntrainloader = DataLoader(trainset, batch_size=8, shuffle=True)\nvalidloader = DataLoader(validset, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:23:13.375481Z","iopub.execute_input":"2023-06-24T18:23:13.375883Z","iopub.status.idle":"2023-06-24T18:23:13.390318Z","shell.execute_reply.started":"2023-06-24T18:23:13.375846Z","shell.execute_reply":"2023-06-24T18:23:13.389472Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer):\n    \"\"\"\n    Save model checkpoint.\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    \"\"\"\n    state = {'epoch': epoch,\n             'model': model,\n             'optimizer': optimizer}\n    filename = 'checkpoint.pth.tar'\n    torch.save(state, filename)\n    \ndef adjust_learning_rate(optimizer, scale):\n    \"\"\"\n    Scale learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param scale: factor to multiply learning rate with.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * scale\n    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:23:13.392488Z","iopub.execute_input":"2023-06-24T18:23:13.392838Z","iopub.status.idle":"2023-06-24T18:23:14.896003Z","shell.execute_reply.started":"2023-06-24T18:23:13.392807Z","shell.execute_reply":"2023-06-24T18:23:14.892889Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-06-24T18:23:14.898909Z","iopub.execute_input":"2023-06-24T18:23:14.899642Z","iopub.status.idle":"2023-06-24T18:23:14.991072Z","shell.execute_reply.started":"2023-06-24T18:23:14.899604Z","shell.execute_reply":"2023-06-24T18:23:14.988991Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"criterion = nn.L1Loss()\nscale = 4\nepochs = 25\nprint_every = 5\ntrain_loss = 0\nbatch_num = 0\ndecay_lr_at = 11, 18  # decay learning rate after these many iterations\ndecay_lr_to = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:11:02.546955Z","iopub.execute_input":"2023-06-24T19:11:02.547369Z","iopub.status.idle":"2023-06-24T19:11:02.553774Z","shell.execute_reply.started":"2023-06-24T19:11:02.547334Z","shell.execute_reply":"2023-06-24T19:11:02.552877Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from super_image.trainer_utils import EvalPrediction\nfrom super_image.utils.metrics import compute_metrics\nimport gc","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:11:02.794842Z","iopub.execute_input":"2023-06-24T19:11:02.795430Z","iopub.status.idle":"2023-06-24T19:11:02.800172Z","shell.execute_reply.started":"2023-06-24T19:11:02.795393Z","shell.execute_reply":"2023-06-24T19:11:02.799160Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"/kaggle/working/checkpoint.pth.tar\"\n\nbest_metric = 0\nbest_epoch = 0\n\ndef train(train_loader,valid_loader, model, criterion, optimizer, epoch):\n    \n    global best_metric, best_epoch\n    losses = AverageMeter()\n    \n    for i, (img, label) in enumerate(train_loader):\n        \n        start = time.time()\n\n        img, label = img.to(device), label.to(device)\n        pred = model(img)\n        # print(pred.shape, label.shape)\n        loss = criterion(pred, label)\n        \n        losses.update(loss.item(), img.size(0))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Print status\n        if i % print_every == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Training Time {3:.3f} \\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n                                                                  (time.time()-start)*print_every, loss=losses))\n\n    with torch.no_grad():\n        \n        model.eval()\n        val_losses = AverageMeter()\n        epoch_psnr = AverageMeter()\n        epoch_ssim = AverageMeter()\n        \n        for i, (val_inputs, val_labels) in enumerate(valid_loader):\n            \n            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n            val_preds = model(val_inputs)\n            val_loss = criterion(val_preds, val_labels)\n            \n            val_losses.update(val_loss.item(), val_inputs.size(0))\n            \n            metrics = compute_metrics(EvalPrediction(predictions=val_preds, labels=val_labels), scale=scale)\n\n            epoch_psnr.update(metrics['psnr'], val_inputs.size(0))\n            epoch_ssim.update(metrics['ssim'], val_inputs.size(0))\n\n        print(f'Validation Loss:{val_losses.avg:.2f}      eval psnr: {epoch_psnr.avg:.2f}     ssim: {epoch_ssim.avg:.4f}')\n\n        if epoch_psnr.avg > best_metric:\n            best_epoch = epoch\n            best_metric = epoch_psnr.avg\n\n            print(f'best epoch: {epoch}, psnr: {epoch_psnr.avg:.6f}, ssim: {epoch_ssim.avg:.6f}')\n            \n            # Save checkpoint\n            print(\"Saving checkpoint epoch:\", epoch)\n            save_checkpoint(epoch, model, optimizer)\n\n#         print('Epoch : {}/{}'.format(epoch_num, epochs))\n#         print('Training Loss : {:.4f}'.format(losses.avg))\n#         print('Validation Loss: {:.4f}'.format(val_losses.avg))","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:11:03.074761Z","iopub.execute_input":"2023-06-24T19:11:03.075340Z","iopub.status.idle":"2023-06-24T19:11:03.090377Z","shell.execute_reply.started":"2023-06-24T19:11:03.075303Z","shell.execute_reply":"2023-06-24T19:11:03.089345Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Training.\n    \"\"\"\n    global start_epoch, label_map, epoch, checkpoint, decay_lr_at, optimizer, criterion, scale\n\n    # Initialize model or load checkpoint\n    if checkpoint is None:\n        config = EdsrConfig(\n        scale=scale,                               \n        n_resblocks=32,\n        n_feats=256\n    )\n        start_epoch = 0\n        model = edsr(config)\n        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n        \n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint['epoch'] + 1\n        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n        model = checkpoint['model']\n        optimizer = checkpoint['optimizer']\n\n    # Move to default device\n    model = model.to(device)\n    criterion = criterion\n\n    print(\"Number of epochs: \", epochs)\n    \n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # Decay learning rate at particular epochs\n        if epoch in decay_lr_at:\n            adjust_learning_rate(optimizer, decay_lr_to)\n\n        # One epoch's training\n        train(train_loader=trainloader,\n              valid_loader = validloader,\n              model=model,\n              criterion=criterion,\n              optimizer=optimizer,\n              epoch=epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:11:04.063802Z","iopub.execute_input":"2023-06-24T19:11:04.064172Z","iopub.status.idle":"2023-06-24T19:11:04.076143Z","shell.execute_reply.started":"2023-06-24T19:11:04.064127Z","shell.execute_reply":"2023-06-24T19:11:04.075110Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:11:04.559609Z","iopub.execute_input":"2023-06-24T19:11:04.559962Z","iopub.status.idle":"2023-06-24T19:20:01.158707Z","shell.execute_reply.started":"2023-06-24T19:11:04.559931Z","shell.execute_reply":"2023-06-24T19:20:01.157051Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"\nLoaded checkpoint from epoch 9.\n\nNumber of epochs:  25\nEpoch: [9][0/100]\tTraining Time 1.550 \tLoss 0.0379 (0.0379)\t\nEpoch: [9][5/100]\tTraining Time 1.476 \tLoss 0.0215 (0.0285)\t\nEpoch: [9][10/100]\tTraining Time 1.468 \tLoss 0.0307 (0.0299)\t\nEpoch: [9][15/100]\tTraining Time 1.469 \tLoss 0.0287 (0.0310)\t\nEpoch: [9][20/100]\tTraining Time 1.467 \tLoss 0.0387 (0.0330)\t\nEpoch: [9][25/100]\tTraining Time 1.470 \tLoss 0.0300 (0.0319)\t\nEpoch: [9][30/100]\tTraining Time 1.471 \tLoss 0.0324 (0.0324)\t\nEpoch: [9][35/100]\tTraining Time 1.482 \tLoss 0.0314 (0.0326)\t\nEpoch: [9][40/100]\tTraining Time 1.469 \tLoss 0.0389 (0.0324)\t\nEpoch: [9][45/100]\tTraining Time 1.469 \tLoss 0.0320 (0.0326)\t\nEpoch: [9][50/100]\tTraining Time 1.465 \tLoss 0.0300 (0.0321)\t\nEpoch: [9][55/100]\tTraining Time 1.467 \tLoss 0.0368 (0.0316)\t\nEpoch: [9][60/100]\tTraining Time 1.485 \tLoss 0.0316 (0.0320)\t\nEpoch: [9][65/100]\tTraining Time 1.499 \tLoss 0.0283 (0.0323)\t\nEpoch: [9][70/100]\tTraining Time 1.472 \tLoss 0.0262 (0.0328)\t\nEpoch: [9][75/100]\tTraining Time 1.466 \tLoss 0.0384 (0.0335)\t\nEpoch: [9][80/100]\tTraining Time 1.472 \tLoss 0.0391 (0.0335)\t\nEpoch: [9][85/100]\tTraining Time 1.472 \tLoss 0.0333 (0.0336)\t\nEpoch: [9][90/100]\tTraining Time 1.472 \tLoss 0.0279 (0.0335)\t\nEpoch: [9][95/100]\tTraining Time 1.466 \tLoss 0.0274 (0.0333)\t\nValidation Loss:0.03      eval psnr: 28.59     ssim: 0.7998\nbest epoch: 9, psnr: 28.585546, ssim: 0.799758\nSaving checkpoint epoch: 9\nEpoch: [10][0/100]\tTraining Time 1.477 \tLoss 0.0254 (0.0254)\t\nEpoch: [10][5/100]\tTraining Time 1.470 \tLoss 0.0492 (0.0374)\t\nEpoch: [10][10/100]\tTraining Time 1.489 \tLoss 0.0312 (0.0364)\t\nEpoch: [10][15/100]\tTraining Time 1.475 \tLoss 0.0311 (0.0362)\t\nEpoch: [10][20/100]\tTraining Time 1.469 \tLoss 0.0199 (0.0366)\t\nEpoch: [10][25/100]\tTraining Time 1.475 \tLoss 0.0291 (0.0360)\t\nEpoch: [10][30/100]\tTraining Time 1.472 \tLoss 0.0221 (0.0347)\t\nEpoch: [10][35/100]\tTraining Time 1.473 \tLoss 0.0343 (0.0336)\t\nEpoch: [10][40/100]\tTraining Time 1.466 \tLoss 0.0412 (0.0333)\t\nEpoch: [10][45/100]\tTraining Time 1.474 \tLoss 0.0366 (0.0335)\t\nEpoch: [10][50/100]\tTraining Time 1.464 \tLoss 0.0401 (0.0334)\t\nEpoch: [10][55/100]\tTraining Time 1.470 \tLoss 0.0360 (0.0333)\t\nEpoch: [10][60/100]\tTraining Time 1.473 \tLoss 0.0464 (0.0337)\t\nEpoch: [10][65/100]\tTraining Time 1.461 \tLoss 0.0264 (0.0334)\t\nEpoch: [10][70/100]\tTraining Time 1.549 \tLoss 0.0270 (0.0330)\t\nEpoch: [10][75/100]\tTraining Time 1.470 \tLoss 0.0404 (0.0329)\t\nEpoch: [10][80/100]\tTraining Time 1.474 \tLoss 0.0171 (0.0329)\t\nEpoch: [10][85/100]\tTraining Time 1.478 \tLoss 0.0314 (0.0327)\t\nEpoch: [10][90/100]\tTraining Time 1.473 \tLoss 0.0349 (0.0327)\t\nEpoch: [10][95/100]\tTraining Time 1.472 \tLoss 0.0247 (0.0328)\t\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[33], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     adjust_learning_rate(optimizer, decay_lr_to)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# One epoch's training\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[32], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (val_inputs, val_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_loader):\n\u001b[1;32m     42\u001b[0m     val_inputs, val_labels \u001b[38;5;241m=\u001b[39m val_inputs\u001b[38;5;241m.\u001b[39mto(device), val_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 43\u001b[0m     val_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(val_preds, val_labels)\n\u001b[1;32m     46\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mupdate(val_loss\u001b[38;5;241m.\u001b[39mitem(), val_inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[14], line 77\u001b[0m, in \u001b[0;36medsr.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/pixelshuffle.py:54\u001b[0m, in \u001b[0;36mPixelShuffle.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_shuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupscale_factor\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.64 GiB (GPU 0; 15.90 GiB total capacity; 5.48 GiB already allocated; 2.59 GiB free; 12.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.64 GiB (GPU 0; 15.90 GiB total capacity; 5.48 GiB already allocated; 2.59 GiB free; 12.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:10:30.128534Z","iopub.status.idle":"2023-06-24T19:10:30.129096Z","shell.execute_reply.started":"2023-06-24T19:10:30.128849Z","shell.execute_reply":"2023-06-24T19:10:30.128877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n\n!tar -czf checkpoint_ssd300.pth.tar \n\nfrom IPython.display import FileLink\n\nFileLink(r'checkpoint.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2023-06-24T19:22:30.150801Z","iopub.execute_input":"2023-06-24T19:22:30.151232Z","iopub.status.idle":"2023-06-24T19:22:31.190582Z","shell.execute_reply.started":"2023-06-24T19:22:30.151199Z","shell.execute_reply":"2023-06-24T19:22:31.189286Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"tar: Cowardly refusing to create an empty archive\nTry 'tar --help' or 'tar --usage' for more information.\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoint.pth.tar","text/html":"<a href='checkpoint.pth.tar' target='_blank'>checkpoint.pth.tar</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
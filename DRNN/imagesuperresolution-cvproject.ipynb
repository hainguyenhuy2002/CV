{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm\nfrom torchvision.transforms.functional import resize, InterpolationMode\n\nimport argparse\nimport os\nimport copy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-10T14:18:52.184830Z","iopub.execute_input":"2023-07-10T14:18:52.186007Z","iopub.status.idle":"2023-07-10T14:18:52.193830Z","shell.execute_reply.started":"2023-07-10T14:18:52.185961Z","shell.execute_reply":"2023-07-10T14:18:52.192488Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"https://github.com/yjn870/DRRN-pytorch","metadata":{}},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = 'tungbinhthuong'\nos.environ['KAGGLE_KEY'] = 'fd622c39c04da294c5e392c7f76ec1a7'","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:53.267780Z","iopub.execute_input":"2023-07-10T14:18:53.269031Z","iopub.status.idle":"2023-07-10T14:18:53.274770Z","shell.execute_reply.started":"2023-07-10T14:18:53.268985Z","shell.execute_reply":"2023-07-10T14:18:53.273291Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# !kaggle kernels output tungbinhthuong/imagesuperresolution-cvproject -p /kaggle/working/best_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n!pip install super-image\n!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args:\n    def __init__(self, B, U, num_features, \n                 weights_file, eval_scale, \n                 lr, batch_size, num_workers,\n                 clip_grad,\n                 num_epochs,\n                 seed,\n                 outputs_dir,\n                    eval = True):\n        self.B = B\n        self.U = U\n        self.num_features = num_features\n        self.weights_file = weights_file\n        self.lr = lr\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.outputs_dir = os.path.join(outputs_dir, str(eval_scale))\n        isExist = os.path.exists(self.outputs_dir)\n        if not isExist:\n            os.makedirs(self.outputs_dir)\n            print(\"The new directory is created!\")\n            \n        self.clip_grad = clip_grad\n        self.num_epochs = num_epochs\n        self.seed = seed\n        self.eval = eval\n        self.eval_scale = eval_scale\n\nargs = Args(B = 1, U = 9, num_features = 32, \n            lr = 0.01, weights_file = None, \n            clip_grad = 0.01, num_epochs = 50, \n            num_workers=2, seed = 123, \n            outputs_dir = \"/kaggle/working/\", \n            batch_size=64, eval_scale = 4)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:56.369705Z","iopub.execute_input":"2023-07-10T14:18:56.370141Z","iopub.status.idle":"2023-07-10T14:18:56.382332Z","shell.execute_reply.started":"2023-07-10T14:18:56.370108Z","shell.execute_reply":"2023-07-10T14:18:56.381164Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(ConvLayer, self).__init__()\n        self.module = nn.Sequential(\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2, bias=False)\n        )\n\n    def forward(self, x):\n        return self.module(x)\n\n\nclass ResidualUnit(nn.Module):\n    def __init__(self, num_features):\n        super(ResidualUnit, self).__init__()\n        self.module = nn.Sequential(\n            ConvLayer(num_features, num_features),\n            ConvLayer(num_features, num_features)\n        )\n\n    def forward(self, h0, x):\n        return h0 + self.module(x)\n\n\nclass RecursiveBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, U):\n        super(RecursiveBlock, self).__init__()\n        self.U = U\n        self.h0 = ConvLayer(in_channels, out_channels)\n        self.ru = ResidualUnit(out_channels)\n\n    def forward(self, x):\n        h0 = self.h0(x)\n        x = h0\n        for i in range(self.U):\n            x = self.ru(h0, x)\n        return x\n\n\nclass DRRN(nn.Module):\n    def __init__(self, B, U, num_channels=1, num_features=128):\n        super(DRRN, self).__init__()\n        self.rbs = nn.Sequential(*[RecursiveBlock(num_channels if i == 0 else num_features, num_features, U) for i in range(B)])\n        self.rec = ConvLayer(num_features, num_channels)\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        residual = x\n        x = self.rbs(x)\n        x = self.rec(x)\n        x += residual\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:56.690565Z","iopub.execute_input":"2023-07-10T14:18:56.691628Z","iopub.status.idle":"2023-07-10T14:18:56.708278Z","shell.execute_reply.started":"2023-07-10T14:18:56.691575Z","shell.execute_reply":"2023-07-10T14:18:56.707098Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:57.114507Z","iopub.execute_input":"2023-07-10T14:18:57.115006Z","iopub.status.idle":"2023-07-10T14:18:57.169281Z","shell.execute_reply.started":"2023-07-10T14:18:57.114966Z","shell.execute_reply":"2023-07-10T14:18:57.167999Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:57.556069Z","iopub.execute_input":"2023-07-10T14:18:57.557144Z","iopub.status.idle":"2023-07-10T14:18:57.564856Z","shell.execute_reply.started":"2023-07-10T14:18:57.557096Z","shell.execute_reply":"2023-07-10T14:18:57.563414Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = DRRN(B=args.B, U=args.U, num_features=args.num_features, num_channels=3)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:58.030437Z","iopub.execute_input":"2023-07-10T14:18:58.031612Z","iopub.status.idle":"2023-07-10T14:18:58.041028Z","shell.execute_reply.started":"2023-07-10T14:18:58.031572Z","shell.execute_reply":"2023-07-10T14:18:58.039948Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:58.510139Z","iopub.execute_input":"2023-07-10T14:18:58.512221Z","iopub.status.idle":"2023-07-10T14:18:58.519185Z","shell.execute_reply.started":"2023-07-10T14:18:58.512172Z","shell.execute_reply":"2023-07-10T14:18:58.517982Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"DRRN(\n  (rbs): Sequential(\n    (0): RecursiveBlock(\n      (h0): ConvLayer(\n        (module): Sequential(\n          (0): ReLU(inplace=True)\n          (1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (ru): ResidualUnit(\n        (module): Sequential(\n          (0): ConvLayer(\n            (module): Sequential(\n              (0): ReLU(inplace=True)\n              (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            )\n          )\n          (1): ConvLayer(\n            (module): Sequential(\n              (0): ReLU(inplace=True)\n              (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n            )\n          )\n        )\n      )\n    )\n  )\n  (rec): ConvLayer(\n    (module): Sequential(\n      (0): ReLU(inplace=True)\n      (1): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import PIL.Image as pil_image\nimport numpy as np\nimport torch\nimport cv2\n\n\ndef load_image(path):\n    return pil_image.open(path).convert('RGB')\n\n\ndef generate_lr(image, scale):\n    image = image.resize((image.width // scale, image.height // scale), resample=pil_image.BICUBIC)\n    image = image.resize((image.width * scale, image.height * scale), resample=pil_image.BICUBIC)\n    return image\n\n\ndef modcrop(image, modulo):\n    w = image.width - image.width % modulo\n    h = image.height - image.height % modulo\n    return image.crop((0, 0, w, h))\n\n\ndef generate_patch(image, patch_size, stride):\n    for i in range(0, image.height - patch_size + 1, stride):\n        for j in range(0, image.width - patch_size + 1, stride):\n            yield image.crop((j, i, j + patch_size, i + patch_size))\n\n\ndef image_to_array(image):\n    return np.array(image).transpose((2, 0, 1))\n\ndef normalize(x):\n    return x / 255.0\n\ndef rgb_to_y(img, dim_order='hwc'):\n    if dim_order == 'hwc':\n        return 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n    else:\n        return 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n\ndef load_weights(model, path):\n    state_dict = model.state_dict()\n    for n, p in torch.load(path, map_location=lambda storage, loc: storage).items():\n        if n in state_dict.keys():\n            state_dict[n].copy_(p)\n        else:\n            raise KeyError(n)\n    return model\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:58.912611Z","iopub.execute_input":"2023-07-10T14:18:58.913371Z","iopub.status.idle":"2023-07-10T14:18:58.934439Z","shell.execute_reply.started":"2023-07-10T14:18:58.913334Z","shell.execute_reply":"2023-07-10T14:18:58.933008Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def convert_rgb_to_y(img, dim_order='hwc'):\n    if dim_order == 'hwc':\n        return 16. + (64.738 * img[..., 0] + 129.057 * img[..., 1] + 25.064 * img[..., 2]) / 256.\n    else:\n        return 16. + (64.738 * img[0] + 129.057 * img[1] + 25.064 * img[2]) / 256.\n\n\ndef denormalize(img):\n    img = img.mul(255.0).clamp(0.0, 255.0)\n    return img\n\ndef calc_psnr(img1, img2, max=255.0):\n    return 10. * ((max ** 2) / ((img1 - img2) ** 2).mean()).log10()\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef calc_ssim(img1, img2):\n    \"\"\"calculate SSIM\n    the same outputs as MATLAB's\n    img1, img2: [0, 255]\n    \"\"\"\n    img1=img1.detach().cpu().numpy()\n    img2 = img2.detach().cpu().numpy()\n    if not img1.shape == img2.shape:\n        raise ValueError('Input images must have the same dimensions.')\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1, img2))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError('Wrong input image dimensions.')\n\ndef compute_metrics(eval_prediction, scale):\n    preds = eval_prediction.predictions\n    labels = eval_prediction.labels\n\n    # from piq import ssim, psnr\n    # print(psnr(denormalize(preds), denormalize(labels), data_range=255.),\n    #       ssim(denormalize(preds), denormalize(labels), data_range=255.))\n\n    # original = preds[0][0][0][0]\n\n    preds = convert_rgb_to_y(denormalize(preds.squeeze(0)), dim_order='chw')\n    labels = convert_rgb_to_y(denormalize(labels.squeeze(0)), dim_order='chw')\n\n    # print(preds[0][0], original * 255.)\n\n    preds = preds[scale:-scale, scale:-scale]\n    labels = labels[scale:-scale, scale:-scale]\n\n    # print(calc_psnr(preds, labels), calc_ssim(preds, labels))\n\n    return {\n        'psnr': calc_psnr(preds, labels),\n        'ssim': calc_ssim(preds, labels)\n    }","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:59.458804Z","iopub.execute_input":"2023-07-10T14:18:59.460213Z","iopub.status.idle":"2023-07-10T14:18:59.482990Z","shell.execute_reply.started":"2023-07-10T14:18:59.460162Z","shell.execute_reply":"2023-07-10T14:18:59.481720Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\nfrom typing import NamedTuple, Tuple, Union\n\nclass EvalPrediction(NamedTuple):\n    \"\"\"\n    Evaluation output (always contains labels), to be used to compute metrics.\n    Parameters:\n        predictions (:obj:`np.ndarray`): Predictions of the model.\n        labels (:obj:`np.ndarray`): Targets to be matched.\n    \"\"\"\n\n    predictions: Union[np.ndarray, Tuple[np.ndarray]]\n    labels: np.ndarray\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:18:59.894422Z","iopub.execute_input":"2023-07-10T14:18:59.894855Z","iopub.status.idle":"2023-07-10T14:18:59.902607Z","shell.execute_reply.started":"2023-07-10T14:18:59.894826Z","shell.execute_reply":"2023-07-10T14:18:59.901163Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### LOAD Div2k Training dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x{}'.format(args.eval_scale), split='train')                          # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x{}'.format(args.eval_scale), split='validation'))      # prepare the eval dataset for the PyTorch DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:19:00.758507Z","iopub.execute_input":"2023-07-10T14:19:00.758889Z","iopub.status.idle":"2023-07-10T14:19:01.656162Z","shell.execute_reply.started":"2023-07-10T14:19:00.758859Z","shell.execute_reply":"2023-07-10T14:19:01.655047Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# model = DRRN(B=args.B, U=args.U, num_features=args.num_features, num_channels=3).to(device)\n\nif args.weights_file is not None:\n    model = load_weights(model, args.weights_file)\n\n#     train_dataset = TrainDataset(args.train_file)\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size,\n                              shuffle=True,\n                              num_workers=args.num_workers,\n                              pin_memory=True)\n\nif args.eval == True:\n#     eval_dataset = EvalDataset(args.eval_file)\n    eval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n\nbest_weights = copy.deepcopy(model.state_dict())\nbest_epoch = 0\nbest_psnr = 0.0\nmodel.to(device)\n\ncriterion = nn.L1Loss()\n\n# optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\nstep_size = int(len(train_dataset) / args.batch_size * 200)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n\nfor epoch in range(args.num_epochs):\n#     lr = args.lr * (0.5 ** ((epoch + 1) // 10))\n    lr = args.lr * (0.1 ** (epoch // int(num_train_epochs * 0.8)))\n\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    model.train()\n    epoch_losses = AverageMeter()\n\n    with tqdm(total=(len(train_dataset) - len(train_dataset) % args.batch_size), ncols=80) as t:\n        t.set_description('epoch: {}/{}'.format(epoch, args.num_epochs - 1))\n\n        for data in train_dataloader:\n            inputs, labels = data\n            x = resize(inputs, size = (inputs.shape[2] * args.eval_scale, inputs.shape[3] * args.eval_scale), interpolation  =InterpolationMode.BICUBIC)\n            x = x.to(device)\n            \n            labels = labels.to(device)\n\n            preds = model(x)\n\n            loss = criterion(preds, labels) / (2 * len(inputs))\n            epoch_losses.update(loss.item(), len(inputs))\n\n            optimizer.zero_grad()\n            loss.backward()\n\n#             nn.utils.clip_grad.clip_grad_norm_(model.parameters(), args.clip_grad / lr)\n\n            optimizer.step()\n            scheduler.step()\n\n            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg), lr=lr)\n            t.update(len(inputs))\n\n    torch.save(model.state_dict(), os.path.join(args.outputs_dir, 'epoch_{}.pth'.format(epoch)))\n\n    if args.eval == True:\n        model.eval()\n        \n        epoch_psnr = AverageMeter()\n        epoch_ssim = AverageMeter()\n\n        for data in eval_dataloader:\n            inputs, labels = data\n            x = resize(inputs, size = (inputs.shape[2] * args.eval_scale, inputs.shape[3] * args.eval_scale), \n                       interpolation = InterpolationMode.BICUBIC)\n            x = x.to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                preds = model(x)\n\n#             preds = denormalize(preds.squeeze(0).squeeze(0))\n#             labels = denormalize(labels.squeeze(0).squeeze(0))\n            res = compute_metrics(EvalPrediction(predictions=preds, labels=labels), args.eval_scale)\n            \n#             epoch_psnr.update(PSNR(preds, labels, shave_border=args.eval_scale), len(inputs))\n            epoch_psnr.update(res['psnr'], len(inputs))\n            epoch_ssim.update(res['ssim'], len(inputs))\n\n        print('eval psnr: {:.2f}, eval ssim: {:.2f}'.format(epoch_psnr.avg, epoch_ssim.avg))\n\n        if epoch_psnr.avg > best_psnr:\n            best_epoch = epoch\n            best_psnr = epoch_psnr.avg\n            best_psnr_ssim = epoch_ssim.avg\n            best_weights = copy.deepcopy(model.state_dict())\n\nif args.eval ==True:\n    print('best epoch: {}, psnr: {:.2f}, ssim: {:.2f}'.format(best_epoch, best_psnr, best_psnr_ssim))\n    torch.save(best_weights, os.path.join(args.outputs_dir, 'best.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:19:02.425347Z","iopub.execute_input":"2023-07-10T14:19:02.425735Z","iopub.status.idle":"2023-07-10T16:36:52.198649Z","shell.execute_reply.started":"2023-07-10T14:19:02.425704Z","shell.execute_reply":"2023-07-10T16:36:52.197227Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"epoch: 0/49:   0%|                                      | 0/768 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\nepoch: 0/49: : 800it [01:32,  8.69it/s, loss=9686115770608.767578, lr=0.1]      \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 11.55, eval ssim: 0.53\n","output_type":"stream"},{"name":"stderr","text":"epoch: 1/49: : 800it [01:23,  9.56it/s, loss=0.033248, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 22.27, eval ssim: 0.77\n","output_type":"stream"},{"name":"stderr","text":"epoch: 2/49: : 800it [01:26,  9.27it/s, loss=0.000653, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 27.58, eval ssim: 0.77\n","output_type":"stream"},{"name":"stderr","text":"epoch: 3/49: : 800it [01:27,  9.17it/s, loss=0.000259, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 4/49: : 800it [01:25,  9.36it/s, loss=0.000250, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 5/49: : 800it [01:21,  9.86it/s, loss=0.000253, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 6/49: : 800it [01:19, 10.05it/s, loss=0.000246, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 7/49: : 800it [01:19, 10.00it/s, loss=0.000255, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 8/49: : 800it [01:19, 10.05it/s, loss=0.000243, lr=0.1]                  \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 9/49: : 800it [01:19, 10.02it/s, loss=0.000260, lr=0.05]                 \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 10/49: : 800it [01:22,  9.72it/s, loss=0.000254, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 11/49: : 800it [01:19, 10.11it/s, loss=0.000255, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 12/49: : 800it [01:18, 10.19it/s, loss=0.000252, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 13/49: : 800it [01:20,  9.94it/s, loss=0.000247, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 14/49: : 800it [01:28,  9.06it/s, loss=0.000248, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 15/49: : 800it [01:26,  9.27it/s, loss=0.000251, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 16/49: : 800it [01:22,  9.73it/s, loss=0.000253, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 17/49: : 800it [01:29,  8.97it/s, loss=0.000256, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 18/49: : 800it [01:27,  9.19it/s, loss=0.000237, lr=0.05]                \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 19/49: : 800it [01:22,  9.66it/s, loss=0.000250, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 20/49: : 800it [01:20,  9.91it/s, loss=0.000251, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 21/49: : 800it [01:20,  9.94it/s, loss=0.000249, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 22/49: : 800it [01:20,  9.91it/s, loss=0.000254, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 23/49: : 800it [01:19, 10.06it/s, loss=0.000239, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 24/49: : 800it [01:20,  9.98it/s, loss=0.000252, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 25/49: : 800it [01:21,  9.80it/s, loss=0.000257, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 26/49: : 800it [01:20,  9.90it/s, loss=0.000258, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 27/49: : 800it [01:21,  9.86it/s, loss=0.000249, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 28/49: : 800it [01:20,  9.88it/s, loss=0.000250, lr=0.025]               \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 29/49: : 800it [01:18, 10.24it/s, loss=0.000247, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 30/49: : 800it [01:18, 10.24it/s, loss=0.000257, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 31/49: : 800it [01:17, 10.33it/s, loss=0.000256, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 32/49: : 800it [01:21,  9.81it/s, loss=0.000247, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 33/49: : 800it [01:24,  9.43it/s, loss=0.000257, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 34/49: : 800it [01:26,  9.27it/s, loss=0.000250, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 35/49: : 800it [01:24,  9.52it/s, loss=0.000244, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 36/49: : 800it [01:23,  9.59it/s, loss=0.000248, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 37/49: : 800it [01:20,  9.93it/s, loss=0.000243, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 38/49: : 800it [01:19, 10.06it/s, loss=0.000246, lr=0.0125]              \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 39/49: : 800it [01:19, 10.12it/s, loss=0.000253, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 40/49: : 800it [01:18, 10.16it/s, loss=0.000251, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 41/49: : 800it [01:17, 10.28it/s, loss=0.000256, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 42/49: : 800it [01:21,  9.80it/s, loss=0.000252, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 43/49: : 800it [01:21,  9.76it/s, loss=0.000249, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 44/49: : 800it [01:29,  8.93it/s, loss=0.000253, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 45/49: : 800it [01:26,  9.21it/s, loss=0.000245, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 46/49: : 800it [01:27,  9.16it/s, loss=0.000246, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 47/49: : 800it [01:28,  9.08it/s, loss=0.000252, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 48/49: : 800it [01:27,  9.17it/s, loss=0.000261, lr=0.00625]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\n","output_type":"stream"},{"name":"stderr","text":"epoch: 49/49: : 800it [01:25,  9.41it/s, loss=0.000245, lr=0.00313]             \n","output_type":"stream"},{"name":"stdout","text":"eval psnr: 28.27, eval ssim: 0.78\nbest epoch: 4, psnr: 28.27, ssim: 0.78\n","output_type":"stream"}]},{"cell_type":"code","source":"from super_image import ImageLoader\nfrom PIL import Image\nfrom torchvision.transforms import ToTensor, ToPILImage\nimport requests\n\ndef detect_from_url(image_url, model, device = 'cpu'):\n    model.to(device)\n    \n    image = Image.open(requests.get(image_url, stream=True).raw)\n    \n    inputs = ImageLoader.load_image(image)\n    preds = model(inputs.to(device))\n\n    ImageLoader.save_image(preds, './scaled_2x.png')\n    ImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')\n\ndef detect_from_path(image_path, model, device = \"cpu\"):\n    model.to(device)\n    \n    # Load the image from the given image_path\n    image = Image.open(image_path)\n\n    # Load the image into a format that the model can understand\n    inputs = ToTensor()(image).unsqueeze(0)\n\n    # Use the given model to make predictions\n    preds = model(inputs.to(device))\n\n    # Convert the predictions into an image\n    output_image = ToPILImage()(preds.squeeze(0))\n\n    # Save the scaled image and a comparison image\n    output_image.save('./scaled_2x.png')\n    ImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')\n\n# Test\n# url = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\n# detect_from_url(url, model)","metadata":{"execution":{"iopub.status.busy":"2023-07-10T14:17:24.633429Z","iopub.status.idle":"2023-07-10T14:17:24.634222Z","shell.execute_reply.started":"2023-07-10T14:17:24.633952Z","shell.execute_reply":"2023-07-10T14:17:24.633979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}